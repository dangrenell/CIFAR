{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grenell cifar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dangrenell/CIFAR/blob/master/grenell_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QfhNx0RzPsZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm using Tensorflow 2 here because it was recently resleased and I want to learn the conventions of the latest release. I don't think there's much if anything here that won't work with previous versions of Tensorflow. I grabbed the CIFAR-10 data directly from Keras because it was easiest. I experiemented with other libraries (many of which were delete). I left a few in for later experimentation."
      ]
    },
    {
      "metadata": {
        "id": "0sgmy-71_2FL",
        "colab_type": "code",
        "outputId": "5038fcdc-3f2c-4fb8-8383-befcf153182d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "# from tensorflow.keras import utils as np_utils\n",
        "# from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 332.1MB 68kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 12.3MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, google-pasta, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8bywGBwUQdmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm just grabbing the data here. Since each pixel in each channel ranges from 0 to 255. I centered it at the mean and scaled by the standard deviation."
      ]
    },
    {
      "metadata": {
        "id": "ZJxm9BKdazUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = (x_train - x_train.mean()) / (x_train.std() + 1e-8)\n",
        "x_test = (x_test - x_test.mean()) / (x_test.std() + 1e-8)\n",
        "\n",
        "# y_train, y_test = tf.one_hot(y_train, 10), tf.one_hot(y_test, 10)\n",
        "\n",
        "# y_train = np_utils.to_categorical(y_train, 10)\n",
        "# y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     horizontal_flip=True)\n",
        "\n",
        "# # compute quantities required for featurewise normalization\n",
        "# # (std, mean, and principal components if ZCA whitening is applied)\n",
        "# datagen.fit(x_train)\n",
        "\n",
        "# # fits the model on batches with real-time data augmentation:\n",
        "# model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "#                     steps_per_epoch=len(x_train) / 32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BU88gHnqT2ux",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model is built, compiled, and fit in this cell. A relu activation was chosen both for speed and because experiements with other activation functions (leaky ReLU and ELU were tried) led to negligible improvements.\n",
        "\n",
        "A CNN was built as the convolutions capture two dimensional features of the image. The final model was selected after experimenting with the following hyperparameters: number of convolutions layers before pooling, kernel size, number of filters, number of max pooling layers, batch normalization vs dropout, dropout rate, number of convolutional-pooling blocks, number of fully connected layers, and width of fully connected layers.\n",
        "\n",
        "The VGG network architecture has been successful with only using 3x3 convolutional windows. After experimenting with various window sizes and orders I adopted their practice. The number of filters increases in each convolutional-pooling block as the lower layers are capturing simpler features, but as the complexity rises there may be more features to capture. \n",
        "\n",
        "A high dropout rate was used in the fully connected layer to prevent overreliance on any particular ouput from the dense layer. Batch normalization is used after convolutional layers to speed up training. As batch normalization introduces noise no dropout was included at these points. All other parameters were set experimentally.\n",
        "\n",
        "Adam is used to optimize rather than generic stochastic gradient descent as the literature indicate that it is a strong performer. Five epochs were used so as to minimize overfitting on training data. After five epochs the accuracy is already well above the loss. "
      ]
    },
    {
      "metadata": {
        "id": "2RShhDKDb_uh",
        "colab_type": "code",
        "outputId": "f9278e70-6a23-4c3c-88f2-f082d81b0270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# activation = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "activation = 'relu'\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # First set of convolutional layers, followed by max pooling.\n",
        "    tf.keras.layers.Conv2D(input_shape=(32, 32, 3),\n",
        "                           kernel_size = 3,\n",
        "                           filters = 32,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "                           filters = 32,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "\n",
        "    # Convolutional layers with more filters, followed by max pooling.\n",
        "    tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "                           filters = 64,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "                           filters = 64,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    \n",
        "    # Convolutional layers with even more filters, followed by max pooling.\n",
        "    tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "                           filters = 128,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "                           filters = 128,\n",
        "                           padding = 'same',\n",
        "                           activation = activation),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "#     # Convolutional layers with even MORE filters, followed by max pooling.\n",
        "#     tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "#                            filters = 254,\n",
        "#                            padding = 'same',\n",
        "#                            activation = activation),\n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "#     tf.keras.layers.Conv2D(kernel_size = 3,\n",
        "#                            filters = 254,\n",
        "#                            padding = 'same',\n",
        "#                            activation = activation),\n",
        "#     tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "#     tf.keras.layers.BatchNormalization(),\n",
        "    \n",
        "    # Flattened fully connected layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512,\n",
        "                          activation = activation),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "    # Ouput layer\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "             loss = 'sparse_categorical_crossentropy',\n",
        "#              loss = 'categorical_crossentropy',\n",
        "             metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 44s 873us/sample - loss: 1.3699 - accuracy: 0.5291\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 43s 852us/sample - loss: 0.9157 - accuracy: 0.6831\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 42s 839us/sample - loss: 0.7366 - accuracy: 0.7464\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 43s 869us/sample - loss: 0.6132 - accuracy: 0.7909\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 43s 867us/sample - loss: 0.5156 - accuracy: 0.8225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdadc1aecc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "oCIkS0AmUH8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, the model is evaluated on the test set. Accuracy is approximately 79%."
      ]
    },
    {
      "metadata": {
        "id": "qSINsjoEe8Pu",
        "colab_type": "code",
        "outputId": "4524942c-4e02-4561-ee25-d980152b49a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)[1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 322us/sample - loss: 0.6529 - accuracy: 0.7907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7907"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "I37_AJZd-bwA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
